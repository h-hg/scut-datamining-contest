{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"islS6gd-8A78","colab_type":"code","colab":{}},"source":["!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"249wz-4auj_B","colab_type":"code","colab":{}},"source":["!wget https://baidu-nlp.bj.bcebos.com/ERNIE_stable-1.0.1.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLwc5Ma-vEIU","colab_type":"code","colab":{}},"source":["!tar -xzvf ERNIE_stable-1.0.1.tar.gz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSHIWd3Q810Z","colab_type":"code","colab":{}},"source":["!unzip -o chinese_L-12_H-768_A-12.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnYgMaLzIGsp","colab_type":"code","colab":{}},"source":["!7z x '/content/drive/My Drive/baidu_ernie.7z' -r -o/content/baidu_ernie"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b9ZEEigAJD5j","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls52QIiLAAZ7","colab_type":"code","colab":{}},"source":["!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVp26EROXLWR","colab_type":"code","colab":{}},"source":["import jieba\n","jieba.load_userdict(\"/content/userdict.txt\")\n","with open('/content/stopwords.txt') as f:\n","    stopWords = [line.strip() for line in f.readlines()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kN6ZCYyA9wxg","colab_type":"code","outputId":"da95fff4-5034-4c90-e90f-b29902a3abe7","executionInfo":{"status":"ok","timestamp":1574430774882,"user_tz":-480,"elapsed":4397,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":65}},"source":["import pandas as pd\n","from tokenization import BasicTokenizer\n","\n","tokenizer = BasicTokenizer()\n","\n","df = pd.read_csv('/content/bert_remove.csv')\n","# 进行分词处理\n","df['cutted'] = df['comment'].apply(lambda x: tokenizer.tokenize(x))\n","#df['cutted'] = df['comment'].apply(lambda x: jieba.lcut(x))\n","\n","# 准备训练测试数据集\n","train_x = list(df['cutted'][:int(len(df)*0.7)])\n","train_y = list(df['label'][:int(len(df)*0.7)])\n","\n","valid_x = list(df['cutted'][int(len(df)*0.7):int(len(df)*0.85)])#\n","valid_y = list(df['label'][int(len(df)*0.7):int(len(df)*0.85)])#int(len(df)*0.85)\n","\n","test_x = list(df['cutted'][int(len(df)*0.85):])\n","test_y = list(df['label'][int(len(df)*0.85):])"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"POQvOb1-CIgk","colab_type":"code","outputId":"33e198fa-a3bb-460f-a4fa-1827947b8b90","executionInfo":{"status":"ok","timestamp":1574430786635,"user_tz":-480,"elapsed":1202,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(train_x[24:25])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[['送', '了', '1', '个', '小', '时', '，', '到', '了', '东', '西', '都', '冷', '了']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bqta2VTRCjpG","colab_type":"code","colab":{}},"source":["!pip install kashgari-tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNOJMxOWCWId","colab_type":"code","outputId":"3638e55e-2229-47f8-c8e5-8b8f2b003fc5","executionInfo":{"status":"ok","timestamp":1574430830327,"user_tz":-480,"elapsed":5719,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":404}},"source":["import kashgari\n","from kashgari.embeddings import BERTEmbedding\n","from kashgari.tasks.classification import BiLSTM_Model\n","from kashgari.tasks.classification import CNN_Model\n","from kashgari.tasks.classification import KMax_CNN_Model\n","kashgari.config.use_cudnn_cell = True\n","\n","BERT_PATH = '/content/baidu_ernie'\n","\n","# 使用 embedding 初始化模型\n","#model = CNN_Model(embed)\n","hyper = KMax_CNN_Model.get_default_hyper_parameters()\n","print(hyper)\n","#hyper['layer_bi_lstm']['units'] = 32\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n","WARNING:root:\n","╭─────────────────────────────────────────────────────────────────────────╮\n","│ ◎ ○ ○ ░░░░░░░░░░░░░░░░░░░░░  Important Message  ░░░░░░░░░░░░░░░░░░░░░░░░│\n","├─────────────────────────────────────────────────────────────────────────┤\n","│                                                                         │\n","│              We renamed again for consistency and clarity.              │\n","│                   From now on, it is all `kashgari`.                    │\n","│  Changelog: https://github.com/BrikerMan/Kashgari/releases/tag/v1.0.0   │\n","│                                                                         │\n","│         | Backend          | pypi version   | desc           |          │\n","│         | ---------------- | -------------- | -------------- |          │\n","│         | TensorFlow 2.x   | kashgari 2.x.x | coming soon    |          │\n","│         | TensorFlow 1.14+ | kashgari 1.x.x |                |          │\n","│         | Keras            | kashgari 0.x.x | legacy version |          │\n","│                                                                         │\n","╰─────────────────────────────────────────────────────────────────────────╯\n","\n","WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"],"name":"stderr"},{"output_type":"stream","text":["{'spatial_dropout': {'rate': 0.2}, 'conv_0': {'filters': 180, 'kernel_size': 1, 'kernel_initializer': 'normal', 'padding': 'valid', 'activation': 'relu'}, 'conv_1': {'filters': 180, 'kernel_size': 2, 'kernel_initializer': 'normal', 'padding': 'valid', 'activation': 'relu'}, 'conv_2': {'filters': 180, 'kernel_size': 3, 'kernel_initializer': 'normal', 'padding': 'valid', 'activation': 'relu'}, 'conv_3': {'filters': 180, 'kernel_size': 4, 'kernel_initializer': 'normal', 'padding': 'valid', 'activation': 'relu'}, 'maxpool_i4': {'k': 3}, 'merged_tensor': {'axis': 1}, 'dropout': {'rate': 0.6}, 'dense': {'units': 144, 'activation': 'relu'}, 'activation_layer': {'activation': 'softmax'}}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aaLEZqVzg9_m","colab_type":"code","colab":{}},"source":["hyper['conv_1']['kernel_size'] = 3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHjRF7kZgdYH","colab_type":"code","outputId":"c5bc3a41-12fd-4695-f46b-8b69b7e043f2","executionInfo":{"status":"ok","timestamp":1574432503949,"user_tz":-480,"elapsed":1664825,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# 初始化 Embedding\n","embed = BERTEmbedding(BERT_PATH,\n","                     task=kashgari.CLASSIFICATION,\n","                     sequence_length=128)\n","\n","model = KMax_CNN_Model(embed,hyper_parameters=hyper)\n","# 先只训练一轮\n","model.fit(train_x, train_y, valid_x, valid_y, batch_size=32, epochs=10)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:root:seq_len: 128\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","Input-Token (InputLayer)        [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","Input-Segment (InputLayer)      [(None, 128)]        0                                            \n","__________________________________________________________________________________________________\n","Embedding-Token (TokenEmbedding [(None, 128, 768), ( 13824000    Input-Token[0][0]                \n","__________________________________________________________________________________________________\n","Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n","__________________________________________________________________________________________________\n","Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n","                                                                 Embedding-Segment[0][0]          \n","__________________________________________________________________________________________________\n","Embedding-Position (PositionEmb (None, 128, 768)     98304       Embedding-Token-Segment[0][0]    \n","__________________________________________________________________________________________________\n","Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Position[0][0]         \n","__________________________________________________________________________________________________\n","Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Dropout[0][0]          \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     2362368     Embedding-Norm[0][0]             \n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Embedding-Norm[0][0]             \n","                                                                 Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-1-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention-\n","                                                                 Encoder-1-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-1-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n","                                                                 Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-2-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention-\n","                                                                 Encoder-2-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-2-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n","                                                                 Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-3-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention-\n","                                                                 Encoder-3-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-3-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n","                                                                 Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-4-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention-\n","                                                                 Encoder-4-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-4-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n","                                                                 Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-5-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention-\n","                                                                 Encoder-5-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-5-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n","                                                                 Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-6-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention-\n","                                                                 Encoder-6-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-6-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n","                                                                 Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-7-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention-\n","                                                                 Encoder-7-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-7-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n","                                                                 Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-8-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention-\n","                                                                 Encoder-8-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-8-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention[\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n","                                                                 Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-9-FeedForward[0][0]      \n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention-\n","                                                                 Encoder-9-FeedForward-Dropout[0][\n","__________________________________________________________________________________________________\n","Encoder-9-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Dropout  (None, 128, 768)     0           Encoder-10-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Add (Add (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n","                                                                 Encoder-10-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-10-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Dropout  (None, 128, 768)     0           Encoder-11-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Add (Add (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n","                                                                 Encoder-11-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-11-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Dropout  (None, 128, 768)     0           Encoder-12-FeedForward[0][0]     \n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Add (Add (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n","                                                                 Encoder-12-FeedForward-Dropout[0]\n","__________________________________________________________________________________________________\n","Encoder-12-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n","__________________________________________________________________________________________________\n","Encoder-Output (Concatenate)    (None, 128, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n","                                                                 Encoder-10-FeedForward-Norm[0][0]\n","                                                                 Encoder-11-FeedForward-Norm[0][0]\n","                                                                 Encoder-12-FeedForward-Norm[0][0]\n","__________________________________________________________________________________________________\n","non_masking_layer (NonMaskingLa (None, 128, 3072)    0           Encoder-Output[0][0]             \n","__________________________________________________________________________________________________\n","spatial_dropout1d (SpatialDropo (None, 128, 3072)    0           non_masking_layer[0][0]          \n","__________________________________________________________________________________________________\n","conv1d (Conv1D)                 (None, 128, 180)     553140      spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","conv1d_1 (Conv1D)               (None, 126, 180)     1659060     spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","conv1d_2 (Conv1D)               (None, 126, 180)     1659060     spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","conv1d_3 (Conv1D)               (None, 125, 180)     2212020     spatial_dropout1d[0][0]          \n","__________________________________________________________________________________________________\n","k_max_pooling_layer (KMaxPoolin (None, 3, 180)       0           conv1d[0][0]                     \n","                                                                 conv1d_1[0][0]                   \n","                                                                 conv1d_2[0][0]                   \n","                                                                 conv1d_3[0][0]                   \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 540)          0           k_max_pooling_layer[0][0]        \n","                                                                 k_max_pooling_layer[1][0]        \n","                                                                 k_max_pooling_layer[2][0]        \n","                                                                 k_max_pooling_layer[3][0]        \n","__________________________________________________________________________________________________\n","concatenate (Concatenate)       (None, 2160)         0           flatten[0][0]                    \n","                                                                 flatten[1][0]                    \n","                                                                 flatten[2][0]                    \n","                                                                 flatten[3][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 2160)         0           concatenate[0][0]                \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 144)          311184      dropout[0][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 2)            290         dense[0][0]                      \n","==================================================================================================\n","Total params: 105,374,594\n","Trainable params: 6,394,754\n","Non-trainable params: 98,979,840\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.6901 - acc: 0.8858Epoch 1/10\n","219/219 [==============================] - 172s 785ms/step - loss: 0.6871 - acc: 0.8861 - val_loss: 0.1410 - val_acc: 0.9380\n","Epoch 2/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.9438Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.1364 - acc: 0.9440 - val_loss: 0.1077 - val_acc: 0.9587\n","Epoch 3/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.1063 - acc: 0.9570Epoch 1/10\n","219/219 [==============================] - 163s 746ms/step - loss: 0.1060 - acc: 0.9571 - val_loss: 0.1045 - val_acc: 0.9587\n","Epoch 4/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0863 - acc: 0.9686Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0863 - acc: 0.9684 - val_loss: 0.1025 - val_acc: 0.9647\n","Epoch 5/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9712Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0783 - acc: 0.9710 - val_loss: 0.0897 - val_acc: 0.9653\n","Epoch 6/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9732Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0702 - acc: 0.9731 - val_loss: 0.0990 - val_acc: 0.9693\n","Epoch 7/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9735Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0687 - acc: 0.9734 - val_loss: 0.0792 - val_acc: 0.9687\n","Epoch 8/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9768Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0597 - acc: 0.9766 - val_loss: 0.0861 - val_acc: 0.9673\n","Epoch 9/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9809Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0496 - acc: 0.9810 - val_loss: 0.1313 - val_acc: 0.9693\n","Epoch 10/10\n","218/219 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9831Epoch 1/10\n","219/219 [==============================] - 163s 745ms/step - loss: 0.0446 - acc: 0.9829 - val_loss: 0.1018 - val_acc: 0.9700\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f096bf3f518>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"dXI6AkidC6oL","colab_type":"code","outputId":"8fdd277d-0adb-4cad-9f88-d3e50fb72618","executionInfo":{"status":"ok","timestamp":1574432555466,"user_tz":-480,"elapsed":30005,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":182}},"source":["model.evaluate(test_x, test_y, batch_size=512)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.9849    0.9865    0.9857      1256\n","           1     0.9298    0.9221    0.9259       244\n","\n","    accuracy                         0.9760      1500\n","   macro avg     0.9573    0.9543    0.9558      1500\n","weighted avg     0.9759    0.9760    0.9760      1500\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A4t-PAaxY0iI","colab_type":"code","colab":{}},"source":["model.save('bert_model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_r_BoCyRY1cq","colab_type":"code","colab":{}},"source":["import kashgari\n","kashgari.utils.convert_to_saved_model(model, 'tf_bert_model', version=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UG6ceHXb6GOI","colab_type":"code","outputId":"3ee40248-a16e-4427-f899-7eb1dbf73c46","executionInfo":{"status":"ok","timestamp":1574432602536,"user_tz":-480,"elapsed":1166,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["test = pd.read_csv(\"test_bert_remove.csv\")\n","print(test.head())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["                                     id                                            comment\n","0  0011f384-9e54-4fb4-a272-330a6cab6804       糯米团是我小时候的记忆了，吃起还是好吃，只是小时候的油条没有这么硬！油茶也还好！可以试试\n","1  00223e4f-47e1-4fc8-9657-06444a7de9a5  满满的五星好评，口味好，服务好，特别喜欢，昨天第一次买，今天就回购了，买的刨奶，店长问我加腰...\n","2  00225350-c169-435c-84cf-970068df5b12                              好喝！经常会再去买来喝！就是排队的人太多了\n","3  00a3190c-90c1-44c3-b809-7a9b1314cd27                              三个人订的四人餐，菜量大没吃完，问道不错。\n","4  00b3f76e-fda3-42cd-8884-25e03a5dba64  好的一如既往，真真爱上了自助炒饭自助八宝粥自助冰粉！喜欢所有菜和肉，两女一男吃两份两人餐没吃...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xV0zbyxT6-JA","colab_type":"code","outputId":"39dc5efb-072e-467c-c597-042e0cd362de","executionInfo":{"status":"ok","timestamp":1574432610703,"user_tz":-480,"elapsed":1619,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["import numpy as np\n","predict_list = list(test['comment'].apply(lambda x: tokenizer.tokenize(x)))\n","#predict_list = list(test['comment'])\n","np_pre = np.asarray(predict_list)\n","print(np_pre[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['满', '满', '的', '五', '星', '好', '评', '，', '口', '味', '好', '，', '服', '务', '好', '，', '特', '别', '喜', '欢', '，', '昨', '天', '第', '一', '次', '买', '，', '今', '天', '就', '回', '购', '了', '，', '买', '的', '刨', '奶', '，', '店', '长', '问', '我', '加', '腰', '果', '还', '是', '核', '桃', '，', '我', '说', '随', '便', '，', '他', '又', '问', '我', '喜', '欢', '吃', '什', '么', '，', '我', '说', '都', '喜', '欢', '，', '然', '后', '，', '帅', '帅', '的', '店', '长', '都', '给', '我', '加', '了', '，', '超', '赞']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6gj3zjTC-OKL","colab_type":"code","colab":{}},"source":["from kashgari import utils\n","processor = utils.load_processor(model_path='tf_bert_model/1')\n","tensor = processor.process_x_dataset(np_pre)\n","# tensor = [{\n","#    \"Input-Token:0\": i.tolist(),\n","#    \"Input-Segment:0\": np.zeros(i.shape).tolist()\n","# } for i in tensor]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJOjDB8rFYzb","colab_type":"code","outputId":"c77dcbf7-3f74-482b-9863-074e87e91165","executionInfo":{"status":"ok","timestamp":1574432830605,"user_tz":-480,"elapsed":1373,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":201}},"source":["print(tensor[1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[   1  596  596    5  346  604  170  480    4  270  775  170    4  231\n","  112  170    4  169  348  692  811    4 2876  125  131    7  218 1042\n","    4  508  125  113  381  817   15    4 1042    5 4120 1618    4  737\n","   84  358   75  120 1844  228  201   10  592 1397    4   75  178  570\n","  518    4   44  311  358   75  692  811  943  614  356    4   75  178\n","  165  692  811    4  187   49    4 1822 1822    5  737   84  165  416\n","   75  120   15    4  634 1479    2    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CCByszLq7oYY","colab_type":"code","outputId":"1e6e4d20-bf01-4c24-9908-f98bf7661eb3","executionInfo":{"status":"error","timestamp":1574432894663,"user_tz":-480,"elapsed":1224,"user":{"displayName":"Hunter Hwang","photoUrl":"","userId":"17918942437350807778"}},"colab":{"base_uri":"https://localhost:8080/","height":343}},"source":["result = model.predict(tensor)\n","print(f\"{result}\")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"UFuncTypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-2929faa967ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{result}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kashgari/tasks/classification/base_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_data, batch_size, multi_label_threshold, debug_info, predict_kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \"\"\"\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mkashgari\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_object_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_x_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kashgari/embeddings/bert_embedding.py\u001b[0m in \u001b[0;36mprocess_x_dataset\u001b[0;34m(self, data, subset)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_x_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_x_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kashgari/processors/base_processor.py\u001b[0m in \u001b[0;36mprocess_x_dataset\u001b[0;34m(self, data, max_len, subset)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mnumerized_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerize_token_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerized_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/kashgari/processors/classification_processor.py\u001b[0m in \u001b[0;36mnumerize_token_sequences\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bos_eos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_bos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_eos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0munk_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_unk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U11'), dtype('<U11')) -> dtype('<U11')"]}]}]}