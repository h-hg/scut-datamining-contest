# -*- coding: utf-8 -*-
"""bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YrSMxJ4gxEGILei9RjQ-UuatObsCpUji
"""

!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip

!wget https://baidu-nlp.bj.bcebos.com/ERNIE_stable-1.0.1.tar.gz

!tar -xzvf ERNIE_stable-1.0.1.tar.gz

!unzip -o chinese_L-12_H-768_A-12.zip

!7z x '/content/drive/My Drive/baidu_ernie.7z' -r -o/content/baidu_ernie

from google.colab import drive
drive.mount('/content/drive')

!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py

import jieba
jieba.load_userdict("/content/userdict.txt")
with open('/content/stopwords.txt') as f:
    stopWords = [line.strip() for line in f.readlines()]

import pandas as pd
from tokenization import BasicTokenizer

tokenizer = BasicTokenizer()

df = pd.read_csv('/content/bert_remove.csv')
# 进行分词处理
df['cutted'] = df['comment'].apply(lambda x: tokenizer.tokenize(x))
#df['cutted'] = df['comment'].apply(lambda x: jieba.lcut(x))

# 准备训练测试数据集
train_x = list(df['cutted'][:int(len(df)*0.7)])
train_y = list(df['label'][:int(len(df)*0.7)])

valid_x = list(df['cutted'][int(len(df)*0.7):int(len(df)*0.85)])#
valid_y = list(df['label'][int(len(df)*0.7):int(len(df)*0.85)])#int(len(df)*0.85)

test_x = list(df['cutted'][int(len(df)*0.85):])
test_y = list(df['label'][int(len(df)*0.85):])

print(train_x[24:25])

!pip install kashgari-tf

import kashgari
from kashgari.embeddings import BERTEmbedding
from kashgari.tasks.classification import BiLSTM_Model
from kashgari.tasks.classification import CNN_Model
from kashgari.tasks.classification import KMax_CNN_Model
kashgari.config.use_cudnn_cell = True

BERT_PATH = '/content/baidu_ernie'

# 使用 embedding 初始化模型
#model = CNN_Model(embed)
hyper = KMax_CNN_Model.get_default_hyper_parameters()
print(hyper)
#hyper['layer_bi_lstm']['units'] = 32

hyper['conv_1']['kernel_size'] = 3

# 初始化 Embedding
embed = BERTEmbedding(BERT_PATH,
                     task=kashgari.CLASSIFICATION,
                     sequence_length=128)

model = KMax_CNN_Model(embed,hyper_parameters=hyper)
# 先只训练一轮
model.fit(train_x, train_y, valid_x, valid_y, batch_size=32, epochs=10)

model.evaluate(test_x, test_y, batch_size=512)

model.save('bert_model')

import kashgari
kashgari.utils.convert_to_saved_model(model, 'tf_bert_model', version=1)

test = pd.read_csv("test_bert_remove.csv")
print(test.head())

import numpy as np
predict_list = list(test['comment'].apply(lambda x: tokenizer.tokenize(x)))
#predict_list = list(test['comment'])
np_pre = np.asarray(predict_list)
print(np_pre[1])

from kashgari import utils
processor = utils.load_processor(model_path='tf_bert_model/1')
tensor = processor.process_x_dataset(np_pre)
# tensor = [{
#    "Input-Token:0": i.tolist(),
#    "Input-Segment:0": np.zeros(i.shape).tolist()
# } for i in tensor]

print(tensor[1])

result = model.predict(tensor)
print(f"{result}")