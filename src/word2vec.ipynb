{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\h_hg\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 2.190 seconds.\nPrefix dict has been built succesfully.\n"
        }
      ],
      "source": [
        "# preprocess\n",
        "import preprocess\n",
        "preprocess.word_segment('process/train_data_remove_spell.txt','process/train_data_remove_spell_words.csv')\n",
        "preprocess.word_segment('process/test_data_remove_spell.txt','process/test_data_remove_spell_words.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load training data, testing data\n",
        "import preprocess\n",
        "X = preprocess.load_word_segment('process/train_data_remove_spell_words.csv')\n",
        "X_verify = preprocess.load_word_segment('process/test_data_remove_spell_words.csv')\n",
        "\n",
        "def load_one_label(input_path):\n",
        "    labels = None\n",
        "    with open(input_path, mode='r',encoding='utf-8') as f:\n",
        "        labels = [int(label.strip()) for label in f.readlines()]\n",
        "    return labels\n",
        "\n",
        "Y = load_one_label('process/train_labels.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "def save_model(file_path, model):\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "def load_model(file_path):\n",
        "    model = None\n",
        "    with open(file_path, 'rb') as f:\n",
        "        model = pickle.load(f)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_name(prefix, params):\n",
        "    name = prefix\n",
        "    for param in sorted(params.items(), key=lambda pair:pair[0]):\n",
        "        name += \"_\" + param[0] + \"_\" + str(param[1])\n",
        "    name += \".bin\"\n",
        "    return name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Initialization of building word2vec model: 2.356s\n"
        }
      ],
      "source": [
        "# word2vec\n",
        "import os\n",
        "import time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "wv_params = {\"iter\":10, \"min_count\":20,\"size\":20}\n",
        "\n",
        "model_path = \"bin/\" + get_model_name('wv',wv_params)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    word2vec_model = load_model(model_path)\n",
        "else:\n",
        "    word2vec_model = Word2Vec(X, **wv_params)\n",
        "    save_model(model_path, word2vec_model)\n",
        "\n",
        "used_time = time.time() - start_time\n",
        "print(u'Initialization of building word2vec model: %.3fs' %used_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def words_to_vector(words, word2vec_model):\n",
        "    vector_list = [ word2vec_model.wv[word] for word in words if word in word2vec_model]\n",
        "    return np.array(vector_list).mean(axis=0)\n",
        "\n",
        "def word2vec(words_list, word2vec_model):\n",
        "    vec_list = [ words_to_vector(words,word2vec_model) for words in words_list]\n",
        "    \"\"\"\n",
        "    vec_list = []\n",
        "    for i in range(len(words_list)):\n",
        "        vec = words_to_vector(words_list[i],word2vec_model)\n",
        "        if str(vec.shape) != '(20,)': # 20要改成wv_params里面的参数\n",
        "            print(i+1, words_list[i])\n",
        "        vec_list.append(vec)\n",
        "    \"\"\"\n",
        "    return np.array(vec_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "C:\\ProgramData\\scoop\\apps\\python\\current\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n  after removing the cwd from sys.path.\n(10000, 20)\n(2000, 20)\n"
        }
      ],
      "source": [
        "wv_X = word2vec(X, word2vec_model)\n",
        "print(wv_X.shape)\n",
        "wv_X_verify = word2vec(X_verify, word2vec_model)\n",
        "print(wv_X_verify.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def report(y_test, y_pred):\n",
        "    n = np.size(y_test)\n",
        "    y_test = np.array(y_test).reshape((n))\n",
        "    y_pred = np.array(y_pred).reshape((n))\n",
        "    C = confusion_matrix(y_test, y_pred)\n",
        "    TP = C[0][0]\n",
        "    TN = C[0][1]\n",
        "    FP = C[1][0]\n",
        "    FN = C[1][1]\n",
        "\n",
        "    print(\"acc: \", np.sum(y_test == y_pred)/n)\n",
        "    print(\"confusion_matrix\\n\", C)\n",
        "    \n",
        "    print(\"safe\")\n",
        "    P = 1.0 * TP / (FP + TP)\n",
        "    R = 1.0 * TP / (TP + FN)\n",
        "    F1 = 2.0 * P * R / (P + R)\n",
        "    print(\"P: \", P)\n",
        "    print(\"R: \", R)\n",
        "    print(\"F1: \", F1)\n",
        "\n",
        "    print(\"unsafe\")\n",
        "    P = 1.0 * TN / (TN + FN)\n",
        "    R = 1.0 * TN / (TN + FP)\n",
        "    F1 = 2.0 * P * R / (P + R)\n",
        "    print(\"P: \", P)\n",
        "    print(\"R: \", R)\n",
        "    print(\"F1: \", F1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "acc:  0.9215\nconfusion_matrix\n [[1655   55]\n [ 102  188]]\nnon-fraud\nP:  0.941946499715424\nR:  0.8979924036896365\nF1:  0.9194444444444444\nfraud\nP:  0.22633744855967078\nR:  0.3503184713375796\nF1:  0.27499999999999997\nC:\\ProgramData\\scoop\\apps\\python\\current\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(wv_X, Y, test_size=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# logistic Regression\n",
        "logisticRegression_model = LogisticRegression()\n",
        "logisticRegression_model.fit(x_train, y_train)\n",
        "y_pred = logisticRegression_model.predict(x_test)\n",
        "report(y_test, y_pred)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}